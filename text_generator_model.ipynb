{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ODEVODEVODEV_COMP_CREATIVITY.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ],
      "metadata": {
        "id": "k4XAw84wspG9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_chicken = open('chicken.txt', 'rb').read().decode(encoding='utf-8')\n",
        "TEXT_meat = open('meat.txt', 'rb').read().decode(encoding='utf-8')\n",
        "TEXT_fish = open('fish.txt', 'rb').read().decode(encoding='utf-8')\n",
        "TEXT_ingredients = open('ingredients.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# The length of texts is the number of characters in it\n",
        "print(len(TEXT_chicken))\n",
        "print(len(TEXT_meat))\n",
        "print(len(TEXT_fish))\n",
        "print(len(TEXT_ingredients))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8eAnH8HskcC",
        "outputId": "096b5eeb-761a-487f-8c36-a05d195ecdb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000\n",
            "500000\n",
            "500000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(TEXT_fish))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "TEXT_as_int = np.array([char2idx[c] for c in TEXT_fish])\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(TEXT_fish)//(seq_length+1)\n",
        "\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(TEXT_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_TEXT = chunk[:-1]\n",
        "    target_TEXT = chunk[1:]\n",
        "    return input_TEXT, target_TEXT\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "\n",
        "def generate_text(model, start_string,t):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = t\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 35\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "model.save(\"fish_model.h5\")\n",
        "\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "text_file_f = generate_text(model, start_string=u\" \",t=0.1)\n"
      ],
      "metadata": {
        "id": "NqC4GRqGMecz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82802984-c1c2-47fb-a17d-6be7a441da53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n",
            "Epoch 1/35\n",
            "77/77 [==============================] - 13s 131ms/step - loss: 2.8172\n",
            "Epoch 2/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 1.9814\n",
            "Epoch 3/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 1.5906\n",
            "Epoch 4/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 1.3147\n",
            "Epoch 5/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 1.1492\n",
            "Epoch 6/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 1.0402\n",
            "Epoch 7/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.9619\n",
            "Epoch 8/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.8966\n",
            "Epoch 9/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.8438\n",
            "Epoch 10/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.7963\n",
            "Epoch 11/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.7503\n",
            "Epoch 12/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.7071\n",
            "Epoch 13/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.6647\n",
            "Epoch 14/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.6244\n",
            "Epoch 15/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5864\n",
            "Epoch 16/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.5511\n",
            "Epoch 17/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.5181\n",
            "Epoch 18/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4823\n",
            "Epoch 19/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4551\n",
            "Epoch 20/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.4290\n",
            "Epoch 21/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4040\n",
            "Epoch 22/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.3838\n",
            "Epoch 23/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.3644\n",
            "Epoch 24/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.3477\n",
            "Epoch 25/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.3343\n",
            "Epoch 26/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.3225\n",
            "Epoch 27/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.3109\n",
            "Epoch 28/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.3005\n",
            "Epoch 29/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.2909\n",
            "Epoch 30/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.2820\n",
            "Epoch 31/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.2761\n",
            "Epoch 32/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.2724\n",
            "Epoch 33/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.2647\n",
            "Epoch 34/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.2596\n",
            "Epoch 35/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "PIjYePOHstev",
        "outputId": "f7392063-9f4e-4727-e5fd-71220a540565"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' and lightly beat egg with 1 tablespoon olive oil. sprinkle with 14 teaspoon sea salt and 14 teaspoon pepper. spread the bread on a baking sheet, spray the bread wedges with olive oil spray., fill a medium saucepan with water and bring to a boil over medium heat. add the onion, fennel, leek, peppercorns, salt, parsley and bay leaf. turn the heat to medium and simmer until the chicken is cooked through, about 10 minutes. drain and transfer the chicken to a cutting board strain the crushed tomatoes, fish stock, seaweed, hot italian peppers, crushed red pepper and season with salt and pepper. spread tapenade on top of steaks. pour orange juice and wine over and around the fish and shrimp. heat olive oil in large nonstick frying pan and add the minced onion. cook until it begins to soften and add the garlic, tomato, and parsley. cook, stirring occasionally, until soft and lightly caramelized. chill in the refrigerator for 1 hour., add the reduced juice mixture to the bowl along with the sca'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(TEXT_meat))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "TEXT_as_int = np.array([char2idx[c] for c in TEXT_meat])\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(TEXT_meat)//(seq_length+1)\n",
        "\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(TEXT_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_TEXT = chunk[:-1]\n",
        "    target_TEXT = chunk[1:]\n",
        "    return input_TEXT, target_TEXT\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "\n",
        "def generate_text(model, start_string,t):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = t\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 35\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "model.save(\"meat_model.h5\")\n",
        "\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "text_file_m = generate_text(model, start_string=u\" \",t=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHa1LoB0yfjx",
        "outputId": "3df55db8-e3bd-4133-eba8-8a8b9e482e45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n",
            "Epoch 1/35\n",
            "77/77 [==============================] - 13s 134ms/step - loss: 2.7921\n",
            "Epoch 2/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 1.9829\n",
            "Epoch 3/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 1.5716\n",
            "Epoch 4/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 1.2940\n",
            "Epoch 5/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 1.1354\n",
            "Epoch 6/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 1.0380\n",
            "Epoch 7/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.9699\n",
            "Epoch 8/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.9172\n",
            "Epoch 9/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.8737\n",
            "Epoch 10/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.8368\n",
            "Epoch 11/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.8056\n",
            "Epoch 12/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.7738\n",
            "Epoch 13/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.7475\n",
            "Epoch 14/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.7204\n",
            "Epoch 15/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.6944\n",
            "Epoch 16/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.6654\n",
            "Epoch 17/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.6431\n",
            "Epoch 18/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.6187\n",
            "Epoch 19/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5958\n",
            "Epoch 20/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5709\n",
            "Epoch 21/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5486\n",
            "Epoch 22/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5258\n",
            "Epoch 23/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5042\n",
            "Epoch 24/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.4843\n",
            "Epoch 25/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.4652\n",
            "Epoch 26/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4473\n",
            "Epoch 27/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4303\n",
            "Epoch 28/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4151\n",
            "Epoch 29/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.4008\n",
            "Epoch 30/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.3892\n",
            "Epoch 31/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 0.3785\n",
            "Epoch 32/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.3679\n",
            "Epoch 33/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.3587\n",
            "Epoch 34/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.3494\n",
            "Epoch 35/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.3422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "M8UA2kNBy394",
        "outputId": "0892afe3-532f-41bd-9a12-472e82c3de6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' for 20 seconds or pour some of the beef stock to come up 23 of the tomatoes with a sharp pairing knife. once the water comes to a boil, reduce to a simmer and cook until slightly browned. add the stock and meat to the pot and cook, stirring occasionally, until the onions are lightly browned and the coriander is fragrant, about 10 seconds per side. transfer the cooked burgers to a rimmed baking sheet. sprinkle with the parsley. season the soup with salt and pepper. serve with toppings, as desired., classic stove top chili recipe, heat a large pot on medium heat, add the garlic and cook until golden brown on both sides, about 30 minutes. let stand 5 minutes. season with salt and pepper. serve with toppings, as desired., classic stove top chili recipe, heat a large pot on medium heat, add onions and garlic to a small bowl. add more olive oil to the pan to melt. sear the veal shanks, turning carefully with tongs, until all sides are a rich brown caramel color. drizzle with a little more oi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(TEXT_chicken))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "TEXT_as_int = np.array([char2idx[c] for c in TEXT_chicken])\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(TEXT_chicken)//(seq_length+1)\n",
        "\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(TEXT_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_TEXT = chunk[:-1]\n",
        "    target_TEXT = chunk[1:]\n",
        "    return input_TEXT, target_TEXT\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "\n",
        "def generate_text(model, start_string,t):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = t\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 35\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "model.save(\"chicken_model.h5\")\n",
        "\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "text_file_c = generate_text(model, start_string=u\" \",t=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrosE6WSy9cc",
        "outputId": "4c9214ed-654a-4ff6-9418-7e51c290d66e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n",
            "Epoch 1/35\n",
            "77/77 [==============================] - 12s 131ms/step - loss: 2.8139\n",
            "Epoch 2/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 1.9306\n",
            "Epoch 3/35\n",
            "77/77 [==============================] - 11s 129ms/step - loss: 1.5130\n",
            "Epoch 4/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 1.2404\n",
            "Epoch 5/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 1.0877\n",
            "Epoch 6/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.9941\n",
            "Epoch 7/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.9298\n",
            "Epoch 8/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.8821\n",
            "Epoch 9/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.8404\n",
            "Epoch 10/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.8065\n",
            "Epoch 11/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.7762\n",
            "Epoch 12/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.7472\n",
            "Epoch 13/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.7200\n",
            "Epoch 14/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.6941\n",
            "Epoch 15/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.6681\n",
            "Epoch 16/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.6446\n",
            "Epoch 17/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.6213\n",
            "Epoch 18/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5978\n",
            "Epoch 19/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5766\n",
            "Epoch 20/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.5547\n",
            "Epoch 21/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.5336\n",
            "Epoch 22/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.5124\n",
            "Epoch 23/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4922\n",
            "Epoch 24/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4757\n",
            "Epoch 25/35\n",
            "77/77 [==============================] - 11s 130ms/step - loss: 0.4570\n",
            "Epoch 26/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.4402\n",
            "Epoch 27/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.4252\n",
            "Epoch 28/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.4108\n",
            "Epoch 29/35\n",
            "77/77 [==============================] - 11s 131ms/step - loss: 0.3975\n",
            "Epoch 30/35\n",
            "77/77 [==============================] - 11s 134ms/step - loss: 0.3867\n",
            "Epoch 31/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.3751\n",
            "Epoch 32/35\n",
            "77/77 [==============================] - 11s 132ms/step - loss: 0.3649\n",
            "Epoch 33/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.3587\n",
            "Epoch 34/35\n",
            "77/77 [==============================] - 11s 133ms/step - loss: 0.3469\n",
            "Epoch 35/35\n",
            "77/77 [==============================] - 11s 134ms/step - loss: 0.3401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "DSKlR1j60VdY",
        "outputId": "22c5dd24-ca0a-4016-e4ad-ca07b620b93f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' minutes. remove to a baking sheet and bake in the preheated oven for 10 minutes. remove from heat and stir in parsley. put potato through cavity along with the remaining 12 teaspoon salt. cover and set aside., for chicken, preheat outdoor grill or grill pan over mediumhigh heat. add the onions and saute until soft, about 5 minutes. stir in the marinade and transfer the remainder to the flour, stirring occasionally. add the peas and cilantro and warm through. garnish with sour cream and whole or chopped cilantro and serve., combine all measured ingredients in a small saucepan and heat the bread crumbs and panko. dip chicken into egg wash, then breadcrumbs and transfer to a sheet pan with oil, bake 35 minutes, until the peas are tender., remove from oven and top with fresh parsley.for the chicken, put the olive oil in a heavybottomed pot over medium heat. add the onions and saute until soft, about 5 minutes. stir in the mint and lemon zest and sprinkle with the allspice. season with salt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def word_extraction(sentence):\n",
        "  ignore = [\"a\", \"the\", \"is\", \"to\", \"until\",\"about\",\"with\",\"and\",\"as\",\"add\",\"on\",\"in\",\"at\",\"of\",\"over\",\"around\",\"knife\",\"sharp\",\"pairing\",\"once\",\"comes\",\"for\",\"minutes\",\"hours\"]\n",
        "  words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
        "  cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "  return str(cleaned_text)\n",
        "\n",
        "\n",
        "ingredients_m = word_extraction(text_file_m)\n",
        "ingredients_c = word_extraction(text_file_c)\n",
        "ingredients_f = word_extraction(text_file_f)\n",
        "##\n"
      ],
      "metadata": {
        "id": "EGuWVHL91IBX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chicken_meal_generator():\n",
        "  output = [ingredients_c, text_file_c]\n",
        "  return output\n",
        "\n",
        "def fish_meal_generator():\n",
        "  output = [ingredients_f, text_file_f]\n",
        "  return output\n",
        "\n",
        "def meat_meal_generator():\n",
        "  output = [ingredients_m, text_file_m]\n",
        "  return output"
      ],
      "metadata": {
        "id": "Ky53eri2YaUL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meat_meal_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPOHePu4ZlzU",
        "outputId": "a5ad4842-afae-487b-dc2c-f8f32cdd774f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"['20', 'seconds', 'or', 'pour', 'some', 'beef', 'stock', 'come', 'up', '23', 'tomatoes', 'water', 'boil', 'reduce', 'simmer', 'cook', 'slightly', 'browned', 'stock', 'meat', 'pot', 'cook', 'stirring', 'occasionally', 'onions', 'are', 'lightly', 'browned', 'coriander', 'fragrant', '10', 'seconds', 'per', 'side', 'transfer', 'cooked', 'burgers', 'rimmed', 'baking', 'sheet', 'sprinkle', 'parsley', 'season', 'soup', 'salt', 'pepper', 'serve', 'toppings', 'desired', 'classic', 'stove', 'top', 'chili', 'recipe', 'heat', 'large', 'pot', 'medium', 'heat', 'garlic', 'cook', 'golden', 'brown', 'both', 'sides', '30', 'let', 'stand', '5', 'season', 'salt', 'pepper', 'serve', 'toppings', 'desired', 'classic', 'stove', 'top', 'chili', 'recipe', 'heat', 'large', 'pot', 'medium', 'heat', 'onions', 'garlic', 'small', 'bowl', 'more', 'olive', 'oil', 'pan', 'melt', 'sear', 'veal', 'shanks', 'turning', 'carefully', 'tongs', 'all', 'sides', 'are', 'rich', 'brown', 'caramel', 'color', 'drizzle', 'little', 'more', 'oi']\",\n",
              " ' for 20 seconds or pour some of the beef stock to come up 23 of the tomatoes with a sharp pairing knife. once the water comes to a boil, reduce to a simmer and cook until slightly browned. add the stock and meat to the pot and cook, stirring occasionally, until the onions are lightly browned and the coriander is fragrant, about 10 seconds per side. transfer the cooked burgers to a rimmed baking sheet. sprinkle with the parsley. season the soup with salt and pepper. serve with toppings, as desired., classic stove top chili recipe, heat a large pot on medium heat, add the garlic and cook until golden brown on both sides, about 30 minutes. let stand 5 minutes. season with salt and pepper. serve with toppings, as desired., classic stove top chili recipe, heat a large pot on medium heat, add onions and garlic to a small bowl. add more olive oil to the pan to melt. sear the veal shanks, turning carefully with tongs, until all sides are a rich brown caramel color. drizzle with a little more oi']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fish_meal_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hiC1tqFZnpZ",
        "outputId": "3bc54923-582d-4e83-b377-8bab7126878f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"['lightly', 'beat', 'egg', '1', 'tablespoon', 'olive', 'oil', 'sprinkle', '14', 'teaspoon', 'sea', 'salt', '14', 'teaspoon', 'pepper', 'spread', 'bread', 'baking', 'sheet', 'spray', 'bread', 'wedges', 'olive', 'oil', 'spray', 'fill', 'medium', 'saucepan', 'water', 'bring', 'boil', 'medium', 'heat', 'onion', 'fennel', 'leek', 'peppercorns', 'salt', 'parsley', 'bay', 'leaf', 'turn', 'heat', 'medium', 'simmer', 'chicken', 'cooked', 'through', '10', 'drain', 'transfer', 'chicken', 'cutting', 'board', 'strain', 'crushed', 'tomatoes', 'fish', 'stock', 'seaweed', 'hot', 'italian', 'peppers', 'crushed', 'red', 'pepper', 'season', 'salt', 'pepper', 'spread', 'tapenade', 'top', 'steaks', 'pour', 'orange', 'juice', 'wine', 'fish', 'shrimp', 'heat', 'olive', 'oil', 'large', 'nonstick', 'frying', 'pan', 'minced', 'onion', 'cook', 'it', 'begins', 'soften', 'garlic', 'tomato', 'parsley', 'cook', 'stirring', 'occasionally', 'soft', 'lightly', 'caramelized', 'chill', 'refrigerator', '1', 'hour', 'reduced', 'juice', 'mixture', 'bowl', 'along', 'sca']\",\n",
              " ' and lightly beat egg with 1 tablespoon olive oil. sprinkle with 14 teaspoon sea salt and 14 teaspoon pepper. spread the bread on a baking sheet, spray the bread wedges with olive oil spray., fill a medium saucepan with water and bring to a boil over medium heat. add the onion, fennel, leek, peppercorns, salt, parsley and bay leaf. turn the heat to medium and simmer until the chicken is cooked through, about 10 minutes. drain and transfer the chicken to a cutting board strain the crushed tomatoes, fish stock, seaweed, hot italian peppers, crushed red pepper and season with salt and pepper. spread tapenade on top of steaks. pour orange juice and wine over and around the fish and shrimp. heat olive oil in large nonstick frying pan and add the minced onion. cook until it begins to soften and add the garlic, tomato, and parsley. cook, stirring occasionally, until soft and lightly caramelized. chill in the refrigerator for 1 hour., add the reduced juice mixture to the bowl along with the sca']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chicken_meal_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0s2vouuZ98G",
        "outputId": "a8f5faed-f09a-42e2-924a-fc43a09114c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"['remove', 'baking', 'sheet', 'bake', 'preheated', 'oven', '10', 'remove', 'from', 'heat', 'stir', 'parsley', 'put', 'potato', 'through', 'cavity', 'along', 'remaining', '12', 'teaspoon', 'salt', 'cover', 'set', 'aside', 'chicken', 'preheat', 'outdoor', 'grill', 'or', 'grill', 'pan', 'mediumhigh', 'heat', 'onions', 'saute', 'soft', '5', 'stir', 'marinade', 'transfer', 'remainder', 'flour', 'stirring', 'occasionally', 'peas', 'cilantro', 'warm', 'through', 'garnish', 'sour', 'cream', 'whole', 'or', 'chopped', 'cilantro', 'serve', 'combine', 'all', 'measured', 'ingredients', 'small', 'saucepan', 'heat', 'bread', 'crumbs', 'panko', 'dip', 'chicken', 'into', 'egg', 'wash', 'then', 'breadcrumbs', 'transfer', 'sheet', 'pan', 'oil', 'bake', '35', 'peas', 'are', 'tender', 'remove', 'from', 'oven', 'top', 'fresh', 'parsley', 'chicken', 'put', 'olive', 'oil', 'heavybottomed', 'pot', 'medium', 'heat', 'onions', 'saute', 'soft', '5', 'stir', 'mint', 'lemon', 'zest', 'sprinkle', 'allspice', 'season', 'salt']\",\n",
              " ' minutes. remove to a baking sheet and bake in the preheated oven for 10 minutes. remove from heat and stir in parsley. put potato through cavity along with the remaining 12 teaspoon salt. cover and set aside., for chicken, preheat outdoor grill or grill pan over mediumhigh heat. add the onions and saute until soft, about 5 minutes. stir in the marinade and transfer the remainder to the flour, stirring occasionally. add the peas and cilantro and warm through. garnish with sour cream and whole or chopped cilantro and serve., combine all measured ingredients in a small saucepan and heat the bread crumbs and panko. dip chicken into egg wash, then breadcrumbs and transfer to a sheet pan with oil, bake 35 minutes, until the peas are tender., remove from oven and top with fresh parsley.for the chicken, put the olive oil in a heavybottomed pot over medium heat. add the onions and saute until soft, about 5 minutes. stir in the mint and lemon zest and sprinkle with the allspice. season with salt']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XaKyHyQqaLeZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}